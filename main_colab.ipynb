{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a7vYQbdzC5cz"
      },
      "outputs": [],
      "source": [
        "!git clone https://github.com/aashishkhimasia/google-research"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p7DZFguVIivQ"
      },
      "outputs": [],
      "source": [
        "%pip install -r /content/google-research/requirements.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T3kqdFzRdZAo"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WROdZoZxdARe"
      },
      "outputs": [],
      "source": [
        "tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n",
        "print('Running on TPU ', tpu.cluster_spec().as_dict())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FONe6QvLJfF1"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xBhOUfEwdGxJ"
      },
      "outputs": [],
      "source": [
        "%cd /content/google-research"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4Px6-Jjhnigv"
      },
      "outputs": [],
      "source": [
        "!tar -xzf \"gs://dissertation-adk/pretrained_emb_voc/vision_vatt_misc_data.tgz\" -C \"/content/google-research/vatt/misc/\"     #[run this cell to extract tar.gz files]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VvTFR12wtuwD"
      },
      "outputs": [],
      "source": [
        "# get the file in, see if that helps with the howto100m"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C_-0dMy6tYUn"
      },
      "outputs": [],
      "source": [
        "!tar -xvzf \"gs://dissertation-adk/pretrained_emb_voc/vision_vatt_misc_data.tgz\" --force-local\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nCyTmhbpIsSf"
      },
      "source": [
        "Main.py from vatt (with edits from LG)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hBKus5W9TtpN"
      },
      "outputs": [],
      "source": [
        "import pprint\n",
        "\n",
        "# from absl import app\n",
        "# from absl import flags\n",
        "from absl import logging\n",
        "\n",
        "from vatt.configs import factory as config_factory\n",
        "from vatt.experiments import finetune\n",
        "from vatt.experiments import pretrain\n",
        "\n",
        "# flags.DEFINE_string('task', 'PRETRAIN', 'PRETRAIN or FINETUNE.')\n",
        "# flags.DEFINE_string('mode', 'train', 'train or eval.')\n",
        "# flags.DEFINE_string('model_dir', None, 'Default path for the experiment.')\n",
        "# flags.DEFINE_string('model_arch', 'Tx_FAC', 'Arch of the model.')\n",
        "# flags.DEFINE_string('override_checkpoint', None,\n",
        "#                     ('Path to a checkpoint for initialization. '\n",
        "#                      'If this is passed, the model is initialized from this '\n",
        "#                      'checkpoint, even if there is a valid latest checkpoint '\n",
        "#                      'inside the model_dir.'))\n",
        "# flags.DEFINE_string('config_file', None,\n",
        "#                     ('Path to a YAML config file containing the dictionary of '\n",
        "#                      'parameters to override the original params defined '\n",
        "#                      'under configs/'))\n",
        "# flags.DEFINE_string('params_override', None,\n",
        "#                     'A safe_dumped str of a dictionary of parameters')\n",
        "# flags.DEFINE_string('strategy_type', 'tpu', 'Type of the distribution strategy')\n",
        "# flags.DEFINE_string('tpu', None, 'Address of the TPU device')\n",
        "\n",
        "\n",
        "# FLAGS = flags.FLAGS\n",
        "\n",
        "\n",
        "def get_params():\n",
        "  \"\"\"Constructs the configuration of the experiment.\"\"\"\n",
        "\n",
        "  task = \"FINETUNE\"  # FLAGS.task\n",
        "  model_arch = \"Tx_FAC\"  # FLAGS.model_arch\n",
        "  params = config_factory.build_experiment_configs(\n",
        "      task=task,\n",
        "      model_arch=model_arch,\n",
        "      )\n",
        "\n",
        "  # if FLAGS.config_file:\n",
        "  #   params.override_from_file(FLAGS.config_file)\n",
        "\n",
        " # if FLAGS.params_override:\n",
        " #    params.override_from_str(FLAGS.params_override)\n",
        "\n",
        "  params.override({\n",
        "      'mode': \"eval\",  # FLAGS.mode,\n",
        "      'model_dir': None, # FLAGS.model_dir,\n",
        "      'checkpoint_path': 'gs://dissertation-adk/checkpoint', #None, # FLAGS.override_checkpoint,\n",
        "      'strategy_config': {'tpu': None, # FLAGS.tpu,\n",
        "                          'distribution_strategy': \"tpu\" # FLAGS.strategy_type\n",
        "                          },\n",
        "  })\n",
        "\n",
        "  return params\n",
        "\n",
        "\n",
        "\n",
        "def main():\n",
        "  # del argv  # Unused.\n",
        "\n",
        "  params = get_params()\n",
        "  logging.info('Model Parameters: %s', pprint.pformat(params.as_dict()))\n",
        "\n",
        "  print(str(params.model_dir))\n",
        "  params.eval.input.name = \"toy_ds\"\n",
        "  if params.task.lower() == 'pretrain':\n",
        "    executor = pretrain.get_executor(params=params)\n",
        "\n",
        "  elif params.task.lower() == 'finetune':\n",
        "    executor = finetune.get_executor(params=params)\n",
        "\n",
        "  else:\n",
        "    raise ValueError('Task not found: %s.' % params.task)\n",
        "\n",
        "  return executor.run(mode=params.mode)\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "  # app.run(main)\n",
        "  main()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G6KmRpeNN17M"
      },
      "outputs": [],
      "source": [
        "%ls"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QtaUWb9yjZ10"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}